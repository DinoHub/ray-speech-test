{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d962b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseSettings, Field\n",
    "\n",
    "class BaseConfig(BaseSettings):\n",
    "    \"\"\"Define any config here.\n",
    "../models/\n",
    "    See here for documentation:\n",
    "    https://pydantic-docs.helpmanual.io/usage/settings/\n",
    "    \"\"\"\n",
    "\n",
    "    num_replicas: int = 1\n",
    "    num_cpus: int = 4\n",
    "    num_gpus: int = 1\n",
    "\n",
    "    # KNative assigns a $PORT environment variable to the container\n",
    "    port: int = Field(default=8080, env=\"PORT\",description=\"App Server Port\")\n",
    "    asr_model_path: str = '../models/stt_en_conformer_ctc_medium.nemo'\n",
    "\n",
    "config = BaseConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063c39e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c1c407c90743b487ac9bc67ee59ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-27 09:35:44 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-05-27 09:35:44 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from nemo.utils import model_utils\n",
    "from nemo.collections.asr.models import ASRModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SpeechRecognizer:\n",
    "    def __init__(self, config) -> ASRModel:\n",
    "\n",
    "        self.device: Union[List[int], int] = [0] if (torch.cuda.is_available() and config.num_gpus >= 1) else 1\n",
    "        self.accelerator: str = 'gpu' if (torch.cuda.is_available() and config.num_gpus >= 1) else 'cpu'\n",
    "        self.map_location: str = torch.device(f'cuda:{self.device[0]}') if self.accelerator == 'gpu' else 'cpu'\n",
    "\n",
    "        # Load model\n",
    "        model_cfg = ASRModel.restore_from(restore_path=config.asr_model_path, return_config=True)\n",
    "        classpath = model_cfg.target  # original class path\n",
    "        imported_class = model_utils.import_class_by_path(classpath)  # type: ASRModel\n",
    "        logging.info(f\"Restoring model : {imported_class.__name__}\")\n",
    "        self.model = imported_class.restore_from(\n",
    "            restore_path=config.asr_model_path, map_location=self.map_location,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(devices=self.device, accelerator=self.accelerator)\n",
    "        self.model.set_trainer(trainer)\n",
    "        self.model = self.model.eval()\n",
    "\n",
    "    ''' Main prediction function '''\n",
    "    def predict(self, audio_tensor: Union[np.ndarray, torch.tensor]) -> str:\n",
    "        \n",
    "        if type(audio_tensor) is np.ndarray:\n",
    "            audio_tensor = torch.tensor(audio_tensor)\n",
    "        \n",
    "        elif type(audio_tensor) is not torch.tensor:\n",
    "            raise TypeError('Input is not an np array or tensor')\n",
    "        \n",
    "        audio_length_tensor = torch.tensor(audio_tensor.shape)\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            logits, logits_len, greedy_predictions = self.model.forward(\n",
    "                            input_signal=audio_tensor.to(self.map_location), \n",
    "                            input_signal_length=audio_length_tensor.to(self.map_location),\n",
    "                        )\n",
    "            \n",
    "            \n",
    "            hypotheses, all_hyp = self.model.decoding.ctc_decoder_predictions_tensor(\n",
    "                            logits, decoder_lengths=logits_len, return_hypotheses=False,\n",
    "                        )\n",
    "\n",
    "        transcription = hypotheses[0]\n",
    "        del logits, logits_len, greedy_predictions\n",
    "            \n",
    "        return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007aa9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-27 09:36:40 mixins:170] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-27 09:36:40 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2023-05-27 09:36:40 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2023-05-27 09:36:40 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-05-27 09:36:40 features:287] PADDING: 0\n",
      "[NeMo I 2023-05-27 09:36:41 audio_preprocessing:517] Numba CUDA SpecAugment kernel is being used\n",
      "[NeMo I 2023-05-27 09:36:41 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /mnt/d/projects/ray-speech-test/models/stt_en_conformer_ctc_medium.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "recog = SpeechRecognizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee3c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "audio, sr = librosa.load('sample.wav', sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f921382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257048de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he came for the singapore summit where his speech was warmly received'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recog.predict(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
